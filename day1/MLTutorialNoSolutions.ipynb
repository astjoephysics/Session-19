{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dba511",
   "metadata": {},
   "source": [
    "# Tutorial for Introduction to ML Lecture\n",
    "\n",
    "version 0.1, September 2023\n",
    "\n",
    "Bryan Scott, CIERA/Northwestern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c6b0f",
   "metadata": {},
   "source": [
    "## Problem 1: Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07842041",
   "metadata": {},
   "source": [
    "A good starting point for Machine Learning is the Bayes classifier. The basic idea is to assign the most probable label to each data point using Bayes theorem, we take:\n",
    "\n",
    "$$\n",
    "p(y | x_n) \\propto p(y)p(x_i, ..., x_n | y)\n",
    "$$\n",
    "\n",
    "where y is a label for a data point and the $x_n$ are the features of the data that we want to use to classify each data point. A $\\textit{Naive} Bayes$ classifier makes an important simplifying assumptions that gives it the name - it assumes that the conditional probabilities are independent, $p(x_i, ..., x_n | y) = p(x_i|y)... p(x_n | y)$. That is, the probability of observing any individual feature doesn't depend on any of the other features. Our task is to construct this classifier from a set of examples we've observed previously and compare it to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40084db8",
   "metadata": {},
   "source": [
    "### Part 0: Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a1d0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e543f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsst_data[0:1000].to_csv('session_19_DC2_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c682ccc",
   "metadata": {},
   "source": [
    "### Loading and splitting the data. \n",
    "\n",
    "Read in the data, then start by selecting the id, fluxes, and object truth type in the lsst data file you've been provided. \n",
    "\n",
    "Once you have selected those, randomly split the data into two arrays, one containing 80% of the data, and a second array containing 20% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "94094b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsst_data = pd.read_csv('./simulated_extragalactic_data.csv') #path to your data\n",
    "\n",
    "data_id = lsst_data['id']\n",
    "flux_u = lsst_data['flux_u']\n",
    "flux_g = lsst_data['flux_g']\n",
    "flux_r = lsst_data['flux_r']\n",
    "flux_i = lsst_data['flux_i']\n",
    "flux_z = lsst_data['flux_z']\n",
    "flux_y = lsst_data['flux_y']\n",
    "truth_type = lsst_data['truth_type']\n",
    "\n",
    "lsst_data_to_classify = pd.DataFrame()\n",
    "lsst_data_to_classify = pd.concat([lsst_data_to_classify, data_id, flux_u, flux_g, flux_r, flux_i, flux_z, flux_y, truth_type], axis=1)\n",
    "\n",
    "# sample 80% randomly\n",
    "random_data_80 = lsst_data_to_classify.sample(frac=0.8)\n",
    "\n",
    "# drop those 80%\n",
    "random_data_20 = lsst_data_to_classify.drop(random_data_80.index)\n",
    "\n",
    "# assign test and train data\n",
    "test_data = random_data_20\n",
    "train_data = random_data_80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2de21",
   "metadata": {},
   "source": [
    "### Part 1: Estimate Class Frequency in the training set\n",
    "\n",
    "One of the ingredients in our classifier is p(y), the unconditional class probabilities. \n",
    "\n",
    "We can get this by counting the number of rows belonging to each class in train_data and dividing by the length of the training data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0186b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_class_probabilities(x_train):\n",
    "    \"\"\"\n",
    "    Computes unconditional class probabilities. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        ints p1, p2: unconditional probability of an element of the training set belonging to class 1\n",
    "    \"\"\"\n",
    "    \n",
    "    p1 = len(x_train[x_train['truth_type'] == 1]) / len(x_train)\n",
    "    p2 = len(x_train[x_train['truth_type'] == 2]) / len(x_train)\n",
    "    return p1, p2\n",
    "\n",
    "p1, p2 = estimate_class_probabilities(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aa268",
   "metadata": {},
   "source": [
    "### Part 2:  Feature Likelihoods\n",
    "\n",
    "We are assuming that the relationship between the classes and feature probabilities are related via:\n",
    "\n",
    "$p(x_i, ..., x_n | y) =  p(x_i|y)... p(x_n | y)$\n",
    "\n",
    "however, we still need to make an assumption about the functional form of the $p(x_i | y)$. As a simple case, we will assume $p(x_i | y)$ follows a Gaussian distribution given by:\n",
    "\n",
    "$$\n",
    "p(x_i | y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y}} \\exp{\\left(-\\frac{(x_i - \\mu)^2}{\\sigma_y^2}\\right)}\n",
    "$$\n",
    "\n",
    "and we will make a maximum likelihood estimate of $\\mu$ and $\\sigma_y$ from the data. This means using empirical estimates $\\bar{x}$ and $\\hat{\\sigma}$ as estimators of the true parameters $\\mu$ and $\\sigma_y$. \n",
    "\n",
    "Write a fitting function that takes the log of the fluxes and returns an estimate of the parameters of the per-feature likelihoods for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "609b65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_feature_likelihood_parameters(x_train, label):\n",
    "    \"\"\"\"\n",
    "    Computes MAP estimates for the class conditional likelihood. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    "        label (int): training labels for the classifier \n",
    " \n",
    "    Returns:\n",
    "        means, stdevs (array): MAP estimates of the Gaussian conditional probability distributions for a specific class\n",
    "    \"\"\"\n",
    "\n",
    "    fluxes = x_train.loc[:, ['flux_u', 'flux_g', 'flux_i', 'flux_z', 'flux_y']]\n",
    "    logfluxes = np.log(fluxes)\n",
    "\n",
    "    i = x_train[x_train['truth_type'] == label].index.values\n",
    "    \n",
    "    means = np.mean(logfluxes.loc[i, fluxes])\n",
    "    stdevs = np.std(logfluxes.loc[i, fluxes])\n",
    "    \n",
    "    return means, stdevs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e88beeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([('f', 'l', 'u', 'x', '_', 'u'), ('f', 'l', 'u', 'x', '_', 'g'),\\n       ('f', 'l', 'u', 'x', '_', 'i'), ('f', 'l', 'u', 'x', '_', 'z'),\\n       ('f', 'l', 'u', 'x', '_', 'y')],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/josephmurtagh/Documents/DSFP/Session-19/day1/Introduction_to_ML_tutorial_no_solutions.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephmurtagh/Documents/DSFP/Session-19/day1/Introduction_to_ML_tutorial_no_solutions.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m per_feature_likelihood_parameters(train_data,\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/josephmurtagh/Documents/DSFP/Session-19/day1/Introduction_to_ML_tutorial_no_solutions.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josephmurtagh/Documents/DSFP/Session-19/day1/Introduction_to_ML_tutorial_no_solutions.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m logfluxes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(fluxes)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josephmurtagh/Documents/DSFP/Session-19/day1/Introduction_to_ML_tutorial_no_solutions.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m i \u001b[39m=\u001b[39m x_train[x_train[\u001b[39m'\u001b[39m\u001b[39mtruth_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m label]\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mvalues\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josephmurtagh/Documents/DSFP/Session-19/day1/Introduction_to_ML_tutorial_no_solutions.ipynb#X44sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m means \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(logfluxes\u001b[39m.\u001b[39;49mloc[i, fluxes])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josephmurtagh/Documents/DSFP/Session-19/day1/Introduction_to_ML_tutorial_no_solutions.ipynb#X44sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m stdevs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(logfluxes\u001b[39m.\u001b[39mloc[i, fluxes])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josephmurtagh/Documents/DSFP/Session-19/day1/Introduction_to_ML_tutorial_no_solutions.ipynb#X44sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m means, stdevs\n",
      "File \u001b[0;32m~/miniconda3/envs/DSFP/lib/python3.8/site-packages/pandas/core/indexing.py:1097\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1096\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1097\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m   1098\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/DSFP/lib/python3.8/site-packages/pandas/core/indexing.py:1287\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[39m# ugly hack for GH #836\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m-> 1287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_take(tup)\n\u001b[1;32m   1289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[0;32m~/miniconda3/envs/DSFP/lib/python3.8/site-packages/pandas/core/indexing.py:1238\u001b[0m, in \u001b[0;36m_LocIndexer._multi_take\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m \u001b[39mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[39mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[39mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[39m# GH 836\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m d \u001b[39m=\u001b[39m {\n\u001b[1;32m   1239\u001b[0m     axis: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1240\u001b[0m     \u001b[39mfor\u001b[39;00m (key, axis) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tup, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_AXIS_ORDERS)\n\u001b[1;32m   1241\u001b[0m }\n\u001b[1;32m   1242\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(d, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/DSFP/lib/python3.8/site-packages/pandas/core/indexing.py:1239\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m \u001b[39mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[39mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[39mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[39m# GH 836\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m d \u001b[39m=\u001b[39m {\n\u001b[0;32m-> 1239\u001b[0m     axis: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1240\u001b[0m     \u001b[39mfor\u001b[39;00m (key, axis) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tup, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_AXIS_ORDERS)\n\u001b[1;32m   1241\u001b[0m }\n\u001b[1;32m   1242\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(d, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/DSFP/lib/python3.8/site-packages/pandas/core/indexing.py:1462\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1459\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1460\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1462\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1464\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/miniconda3/envs/DSFP/lib/python3.8/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DSFP/lib/python3.8/site-packages/pandas/core/indexes/base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5936\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5937\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 5938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   5941\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([('f', 'l', 'u', 'x', '_', 'u'), ('f', 'l', 'u', 'x', '_', 'g'),\\n       ('f', 'l', 'u', 'x', '_', 'i'), ('f', 'l', 'u', 'x', '_', 'z'),\\n       ('f', 'l', 'u', 'x', '_', 'y')],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "per_feature_likelihood_parameters(train_data,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbeb61",
   "metadata": {},
   "source": [
    "### Part 3: MAP Estimates of the Class Probabilities\n",
    "\n",
    "Now that we have the unconditional class probabilities and the parameters of the per feature likelihoods in hand, we can put this all together to build the classifier. Use the methods you have already written to write a function that takes in the training data and returns fit parameters. Once you have done that, write a method that takes the fit parameters as an argument and predicts the class of new (and unseen) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05a17f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classifier\n",
    "\n",
    "# solved \n",
    "\n",
    "def fit(x_train):\n",
    "    \"\"\"\"\n",
    "    Convenience function to perform fitting on the training data\n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std: see documentation for per_feature_likelihood_parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    p1, p2 = estimate_class_probabilities(x_train)\n",
    "    class_1_mean, class_1_std\n",
    "    \n",
    "    return p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01867cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(x_test, class_probability, class_means, class_dev):\n",
    "    \"\"\"\"\n",
    "    Predict method\n",
    "     \n",
    "    Args:\n",
    "        x_test (array): data to perform classification on\n",
    "        class_probability (array): unconditional class probabilities\n",
    "        class_means, class_dev (array): MAP estimates produced by the fit method\n",
    " \n",
    "    Returns:\n",
    "        predict_List (list): class membership predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute probabilities of an element of the test set belonging to class 1 or 2\n",
    "        \n",
    "    for i in range():\n",
    "        if \n",
    "            \n",
    "        if \n",
    "    \n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067d9e3",
   "metadata": {},
   "source": [
    "### Part 4: Metrics\n",
    "\n",
    "After creating a classifier, you now want to evaluate it in terms of how often it correctly and incorrectly classifies the objects in your training set. To do this, we'll design a confusion matrix. A confusion matrix is a matrix whose entries are the counts of the predicted vs actual class. For example, the first entry is the count of objects that are predicted to be of class 1 and actually are of class 1 and so on, while the off-diagonal elements would be instances of class 1 that are predicted to be of class 2, and instances of class 2 that are predicted to be of class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a366c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df_confusion, cmap=):\n",
    "    \"\"\"\n",
    "    \n",
    "    Convenience function to plot the confusion matrix from a pd.crosstab object. Hint: use plt.matshow and choose a sensible color map.\n",
    "    \n",
    "    Args:\n",
    "        df_confusion (pd.crosstab): A pd.crosstab object.\n",
    "        \n",
    "    Returns:\n",
    "        null \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    plt.matshow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c8763",
   "metadata": {},
   "source": [
    "## Problem 2: The Cramer-Rao bound (pen & paper, challenging, optional)\n",
    "\n",
    "As we saw in the lecture, the Cramer-Rao bound is an important result in statistics that has intuitive consequences for many applied problems in ML. The proof of the Cramer-Rao bound can be insightful to work through. \n",
    "\n",
    "The starting point for the proof of the bound is the Cauchy-Schwarz inequality, which can be used to show that:\n",
    "\n",
    "$$\n",
    "[Cov(U, V)]^2 \\le Var(U)Var(V)\n",
    "$$\n",
    "\n",
    "Starting from the definitions that U = T(X), where T(X) is an estimator of some parameter $\\theta$ of the distribution $f(X|\\theta)$ from which the data is sampled, and V = $\\frac{\\partial}{\\partial \\theta} log f(X |\\theta)$. Use the Cauchy-Schwarz inequality to show the Cramer-Rao bound for these choices of U and V. \n",
    "\n",
    "$\\textit{Hint:}$ you will need the fact that the $\\mathbb{E}(V) = 0$, where $\\mathbb{E}$ is the expectation of a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1432f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
