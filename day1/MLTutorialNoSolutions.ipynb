{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dba511",
   "metadata": {},
   "source": [
    "# Tutorial for Introduction to ML Lecture\n",
    "\n",
    "version 0.1, September 2023\n",
    "\n",
    "Bryan Scott, CIERA/Northwestern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c6b0f",
   "metadata": {},
   "source": [
    "## Problem 1: Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07842041",
   "metadata": {},
   "source": [
    "A good starting point for Machine Learning is the Bayes classifier. The basic idea is to assign the most probable label to each data point using Bayes theorem, we take:\n",
    "\n",
    "$$\n",
    "p(y | x_n) \\propto p(y)p(x_i, ..., x_n | y)\n",
    "$$\n",
    "\n",
    "where y is a label for a data point and the $x_n$ are the features of the data that we want to use to classify each data point. A $\\textit{Naive} Bayes$ classifier makes an important simplifying assumptions that gives it the name - it assumes that the conditional probabilities are independent, $p(x_i, ..., x_n | y) = p(x_i|y)... p(x_n | y)$. That is, the probability of observing any individual feature doesn't depend on any of the other features. Our task is to construct this classifier from a set of examples we've observed previously and compare it to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40084db8",
   "metadata": {},
   "source": [
    "### Part 0: Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1d0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e543f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsst_data[0:1000].to_csv('session_19_DC2_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c682ccc",
   "metadata": {},
   "source": [
    "### Loading and splitting the data. \n",
    "\n",
    "Read in the data, then start by selecting the id, fluxes, and object truth type in the lsst data file you've been provided. \n",
    "\n",
    "Once you have selected those, randomly split the data into two arrays, one containing 80% of the data, and a second array containing 20% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94094b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsst_data = pd.read_csv('./session_19_extragalactic_subset.csv') #path to your data\n",
    "\n",
    "data_id = lsst_data['id']\n",
    "flux_u = lsst_data['flux_u']\n",
    "flux_g = lsst_data['flux_g']\n",
    "flux_r = lsst_data['flux_r']\n",
    "flux_i = lsst_data['flux_i']\n",
    "flux_z = lsst_data['flux_z']\n",
    "flux_y = lsst_data['flux_y']\n",
    "truth_type = lsst_data['truth_type']\n",
    "\n",
    "lsst_data_to_classify = pd.DataFrame()\n",
    "lsst_data_to_classify = pd.concat([lsst_data_to_classify, data_id, flux_u, flux_g, flux_r, flux_i, flux_z, flux_y, truth_type], axis=1)\n",
    "random_data_80 = lsst_data_to_classify.sample(frac=0.8)\n",
    "\n",
    "# # train_data = \n",
    "# test_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12f38d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flux_u</th>\n",
       "      <th>flux_g</th>\n",
       "      <th>flux_r</th>\n",
       "      <th>flux_i</th>\n",
       "      <th>flux_z</th>\n",
       "      <th>flux_y</th>\n",
       "      <th>truth_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>41021122263</td>\n",
       "      <td>9.07858</td>\n",
       "      <td>92.1514</td>\n",
       "      <td>265.9440</td>\n",
       "      <td>983.080</td>\n",
       "      <td>1698.520</td>\n",
       "      <td>2180.370</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>41021145667</td>\n",
       "      <td>4241.29000</td>\n",
       "      <td>23858.4000</td>\n",
       "      <td>46478.0000</td>\n",
       "      <td>59495.900</td>\n",
       "      <td>65383.200</td>\n",
       "      <td>69792.900</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>40749406217</td>\n",
       "      <td>532.68600</td>\n",
       "      <td>4860.1800</td>\n",
       "      <td>14496.2000</td>\n",
       "      <td>38958.600</td>\n",
       "      <td>60260.500</td>\n",
       "      <td>74047.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>40970038671</td>\n",
       "      <td>1.75455</td>\n",
       "      <td>16.7219</td>\n",
       "      <td>50.0355</td>\n",
       "      <td>154.926</td>\n",
       "      <td>253.348</td>\n",
       "      <td>318.287</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>41020953141</td>\n",
       "      <td>6.42051</td>\n",
       "      <td>58.4849</td>\n",
       "      <td>174.2350</td>\n",
       "      <td>467.853</td>\n",
       "      <td>723.233</td>\n",
       "      <td>888.376</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>40969790883</td>\n",
       "      <td>3.06467</td>\n",
       "      <td>28.4247</td>\n",
       "      <td>88.2346</td>\n",
       "      <td>190.004</td>\n",
       "      <td>269.319</td>\n",
       "      <td>320.651</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>40252940743</td>\n",
       "      <td>16128.70000</td>\n",
       "      <td>153276.0000</td>\n",
       "      <td>457695.0000</td>\n",
       "      <td>1415010.000</td>\n",
       "      <td>2311540.000</td>\n",
       "      <td>2902200.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>40901249202</td>\n",
       "      <td>35130.00000</td>\n",
       "      <td>104729.0000</td>\n",
       "      <td>157636.0000</td>\n",
       "      <td>184695.000</td>\n",
       "      <td>196499.000</td>\n",
       "      <td>201916.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40749426241</td>\n",
       "      <td>32.49150</td>\n",
       "      <td>329.8190</td>\n",
       "      <td>951.8760</td>\n",
       "      <td>3518.770</td>\n",
       "      <td>6079.700</td>\n",
       "      <td>7804.520</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>40555737954</td>\n",
       "      <td>6.08034</td>\n",
       "      <td>48.9410</td>\n",
       "      <td>171.1450</td>\n",
       "      <td>858.562</td>\n",
       "      <td>1891.180</td>\n",
       "      <td>3183.350</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       flux_u       flux_g       flux_r       flux_i  \\\n",
       "212  41021122263      9.07858      92.1514     265.9440      983.080   \n",
       "347  41021145667   4241.29000   23858.4000   46478.0000    59495.900   \n",
       "897  40749406217    532.68600    4860.1800   14496.2000    38958.600   \n",
       "419  40970038671      1.75455      16.7219      50.0355      154.926   \n",
       "871  41020953141      6.42051      58.4849     174.2350      467.853   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "273  40969790883      3.06467      28.4247      88.2346      190.004   \n",
       "422  40252940743  16128.70000  153276.0000  457695.0000  1415010.000   \n",
       "729  40901249202  35130.00000  104729.0000  157636.0000   184695.000   \n",
       "7    40749426241     32.49150     329.8190     951.8760     3518.770   \n",
       "747  40555737954      6.08034      48.9410     171.1450      858.562   \n",
       "\n",
       "          flux_z       flux_y  truth_type  \n",
       "212     1698.520     2180.370           2  \n",
       "347    65383.200    69792.900           2  \n",
       "897    60260.500    74047.000           2  \n",
       "419      253.348      318.287           2  \n",
       "871      723.233      888.376           2  \n",
       "..           ...          ...         ...  \n",
       "273      269.319      320.651           2  \n",
       "422  2311540.000  2902200.000           2  \n",
       "729   196499.000   201916.000           2  \n",
       "7       6079.700     7804.520           2  \n",
       "747     1891.180     3183.350           2  \n",
       "\n",
       "[800 rows x 8 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data_80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2de21",
   "metadata": {},
   "source": [
    "### Part 1: Estimate Class Frequency in the training set\n",
    "\n",
    "One of the ingredients in our classifier is p(y), the unconditional class probabilities. \n",
    "\n",
    "We can get this by counting the number of rows belonging to each class in train_data and dividing by the length of the training data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0186b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_class_probabilities():\n",
    "    \"\"\"\n",
    "    Computes unconditional class probabilities. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        ints p1, p2: unconditional probability of an element of the training set belonging to class 1\n",
    "    \"\"\"\n",
    "    \n",
    "    p1 = \n",
    "    p2 = \n",
    "    return p1, p2\n",
    "\n",
    "p1, p2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aa268",
   "metadata": {},
   "source": [
    "### Part 2:  Feature Likelihoods\n",
    "\n",
    "We are assuming that the relationship between the classes and feature probabilities are related via:\n",
    "\n",
    "$p(x_i, ..., x_n | y) =  p(x_i|y)... p(x_n | y)$\n",
    "\n",
    "however, we still need to make an assumption about the functional form of the $p(x_i | y)$. As a simple case, we will assume $p(x_i | y)$ follows a Gaussian distribution given by:\n",
    "\n",
    "$$\n",
    "p(x_i | y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y}} \\exp{\\left(-\\frac{(x_i - \\mu)^2}{\\sigma_y^2}\\right)}\n",
    "$$\n",
    "\n",
    "and we will make a maximum likelihood estimate of $\\mu$ and $\\sigma_y$ from the data. This means using empirical estimates $\\bar{x}$ and $\\hat{\\sigma}$ as estimators of the true parameters $\\mu$ and $\\sigma_y$. \n",
    "\n",
    "Write a fitting function that takes the log of the fluxes and returns an estimate of the parameters of the per-feature likelihoods for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "609b65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_feature_likelihood_parameters(x_train, label):\n",
    "    \"\"\"\"\n",
    "    Computes MAP estimates for the class conditional likelihood. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    "        label (int): training labels for the classifier \n",
    " \n",
    "    Returns:\n",
    "        means, stdevs (array): MAP estimates of the Gaussian conditional probability distributions for a specific class\n",
    "    \"\"\"\n",
    "    \n",
    "    means = \n",
    "    stdevs = \n",
    "    \n",
    "    return means, stdevs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbeb61",
   "metadata": {},
   "source": [
    "### Part 3: MAP Estimates of the Class Probabilities\n",
    "\n",
    "Now that we have the unconditional class probabilities and the parameters of the per feature likelihoods in hand, we can put this all together to build the classifier. Use the methods you have already written to write a function that takes in the training data and returns fit parameters. Once you have done that, write a method that takes the fit parameters as an argument and predicts the class of new (and unseen) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05a17f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classifier\n",
    "\n",
    "# solved \n",
    "\n",
    "def fit(x_train):\n",
    "    \"\"\"\"\n",
    "    Convenience function to perform fitting on the training data\n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std: see documentation for per_feature_likelihood_parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute probabilities and MAP estimates of the Gaussian distribution's parameters using the methods you wrote above\n",
    "    \n",
    "    return p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01867cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(x_test, class_probability, class_means, class_dev):\n",
    "    \"\"\"\"\n",
    "    Predict method\n",
    "     \n",
    "    Args:\n",
    "        x_test (array): data to perform classification on\n",
    "        class_probability (array): unconditional class probabilities\n",
    "        class_means, class_dev (array): MAP estimates produced by the fit method\n",
    " \n",
    "    Returns:\n",
    "        predict_List (list): class membership predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute probabilities of an element of the test set belonging to class 1 or 2\n",
    "        \n",
    "    for i in range():\n",
    "        if \n",
    "            \n",
    "        if \n",
    "    \n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067d9e3",
   "metadata": {},
   "source": [
    "### Part 4: Metrics\n",
    "\n",
    "After creating a classifier, you now want to evaluate it in terms of how often it correctly and incorrectly classifies the objects in your training set. To do this, we'll design a confusion matrix. A confusion matrix is a matrix whose entries are the counts of the predicted vs actual class. For example, the first entry is the count of objects that are predicted to be of class 1 and actually are of class 1 and so on, while the off-diagonal elements would be instances of class 1 that are predicted to be of class 2, and instances of class 2 that are predicted to be of class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a366c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df_confusion, cmap=):\n",
    "    \"\"\"\n",
    "    \n",
    "    Convenience function to plot the confusion matrix from a pd.crosstab object. Hint: use plt.matshow and choose a sensible color map.\n",
    "    \n",
    "    Args:\n",
    "        df_confusion (pd.crosstab): A pd.crosstab object.\n",
    "        \n",
    "    Returns:\n",
    "        null \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    plt.matshow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c8763",
   "metadata": {},
   "source": [
    "## Problem 2: The Cramer-Rao bound (pen & paper, challenging, optional)\n",
    "\n",
    "As we saw in the lecture, the Cramer-Rao bound is an important result in statistics that has intuitive consequences for many applied problems in ML. The proof of the Cramer-Rao bound can be insightful to work through. \n",
    "\n",
    "The starting point for the proof of the bound is the Cauchy-Schwarz inequality, which can be used to show that:\n",
    "\n",
    "$$\n",
    "[Cov(U, V)]^2 \\le Var(U)Var(V)\n",
    "$$\n",
    "\n",
    "Starting from the definitions that U = T(X), where T(X) is an estimator of some parameter $\\theta$ of the distribution $f(X|\\theta)$ from which the data is sampled, and V = $\\frac{\\partial}{\\partial \\theta} log f(X |\\theta)$. Use the Cauchy-Schwarz inequality to show the Cramer-Rao bound for these choices of U and V. \n",
    "\n",
    "$\\textit{Hint:}$ you will need the fact that the $\\mathbb{E}(V) = 0$, where $\\mathbb{E}$ is the expectation of a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1432f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
